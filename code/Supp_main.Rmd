---
title: "Supporting_information"
author: "Emma M Atkinson, Bruno S Carturan, Clare P Atkinson, Andrew W Bateman, Katrina Connors, Eric Hertz, Stephanie J Peacock"
# date: "2025-05-14"
output:
  html_document:
    toc: true                  # Adds a table of contents to the document
    toc_float: true           # Makes the table of contents float on the side as the reader scrolls.
    toc_collapsed: true        # Starts the table of contents in a collapsed state.
    toc_depth: 3               # Specifies the depth of headers (e.g., ##, ###) to include in the table of contents.
    number_sections: true      # 
    theme: journal  # lumen
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

```{r, include=FALSE}
rm(list = ls())
graphics.off()

wd <- gsub("code","",getwd())

wd_figures <- paste0(wd,"figures")
wd_data_input <- paste0(wd,"data_input")
wd_data_output <- paste0(wd,"data_output")

library(tidyr)
library(dplyr)
library(paletteer) # https://r-graph-gallery.com/color-palette-finder
library(readxl)
# library(here)
library(scales)
library(AICcmodavg) # for AICc
library(car)
library(MASS)
library(psych)

source("functions.R")
source("colours.R")

figures_print <- F

#
# Import files

#'* Counts and proportions of populations and CUs assessed across regions and species *
filename <- "populationAssessed_catches_data_remove_0s_NAs"
# filename <- "populationAssessed_catches_data_remove_NAs"

#'* Counts and proportions of populations and CUs assessed per species
data_sp <-  read_xlsx(paste0(wd_data_output,"/",filename,".xlsx"), 
                      sheet = "populations_species") |> as.data.frame()

head(data_sp)

filename <- "populationAssessed_catches_data_remove_NAs"
data_sp_0 <-  read_xlsx(paste0(wd_data_output,"/",filename,".xlsx"), 
                        sheet = "populations_species") |> as.data.frame()

#'* Import the catch data
#' The file is the same regardless of filename
catch  <-  read_xlsx(paste0(wd_data_output,"/",filename,".xlsx"), 
                     sheet = "catches_species_total") |> as.data.frame()
head(catch)

#'* Import the landing value per species per kg *
# https://www.pac.dfo-mpo.gc.ca/analyses-econom-analysis/analyses/econ-perspective-salmon-saumon-eng.html

price_sp <- read.csv(paste0(wd_data_input,"/landed-value-valeur-debarquement-eng.csv"), 
                     header = T)
head(price_sp)
colnames(price_sp)[1] <- "Year"
for(c in 2:ncol(price_sp)){
  price_sp[,c] <- as.numeric(gsub("\\$","",price_sp[,c]))
}
```

```{r, include=FALSE}
#' Create the dataset containing the variables needed.
#' Do that for both datasets with and without the 0s

species <- sort(unique(data_sp$species))

data_sp_l <- list(data_sp,data_sp_0)
names(data_sp_l) <- c("data_sp","data_sp_0")

data_l <- list(NA,NA)
names(data_l) <- c("data","data_0")
i <- 1
for(dataset_selected in data_sp_l){
  year_min <- max(min(dataset_selected$year),min(catch$year),min(price_sp$Year))
  year_max <- min(max(dataset_selected$year),max(catch$year),max(price_sp$Year))
  years <- year_min:year_max
  
  col_selected <- c("species","year","count_pop")
  if(any(grepl("region",colnames(dataset_selected)))){
    col_selected <- c("region","species","year","count_pop")
  }
  
  data <- dataset_selected[dataset_selected$year %in% years,col_selected]
  data$count_catch <- NA
  data$catch_kg <- NA
  data$price_kg <- NA
  for(yr in years){
    # yr <- years[1]
    for(sp in unique(dataset_selected$species)){
      # sp <- unique(dataset_selected$species)[1]
      cond <- catch$species == sp & catch$year == yr
      count_catch <- catch$count[cond]
      catch_kg <- catch$wt_kg[cond]
      
      cond <- price_sp$Year == yr
      price_kg <- price_sp[cond,sp]
      
      cond <- data$species == sp & data$year == yr
      data$count_catch[cond] <- count_catch
      data$catch_kg[cond] <- catch_kg
      data$price_kg[cond] <- price_kg
    }
  }
  
  data$species <- factor(data$species,levels = c("Chinook","Chum","Coho","Pink","Sockeye"))
  
  data_l[[i]] <- data
  i <- i + 1
}
```


The is the supporting information for: *Atkinson et al. Monitoring for fisheries or for fish? Declines in monitoring of salmon spawners continue despite a conservation crisis*.

The document contains *(i)* the supplementary figures and tables referred to in the main text, *(ii)* the details of the generalized linear regression analysis conducted to quantify the effect of commercial catch, price on monitoring effort for each species, and *(iii)* a summary of the NuSEDS cleaning procedure.

# Supplementary figures and tables


![fig](`r paste0(wd_figures,"/Number_populations_monitored_species_regions.jpeg")`)
**Figure S1.** Annual number of salmon stream populations monitored over time in NuSEDS after filtering zeros. 

![fig](`r paste0(wd_figures,"/Proportion_populations_monitored_total_regions_species.jpeg")`)
**Figure S2.** Annual proportion of salmon stream populations assessed over time in NuSEDS after filtering zeros.

![fig](`r paste0(wd_figures,"/Number_Pink_populations_regions.jpeg")`)
**Figure S3.** Annual number of pink salmon populations in odd and even years monitored in NuSEDS and broken down by region (panels a through f). Zeros were filtered.

![fig](`r paste0(wd_figures,"/Number_populations_monitored_regions_species_WITH_0s.jpeg")`)
**Figure S4.** Annual number of salmon populations monitored in NuSEDS broken down by region (panels a through i) and by species (Chinook, chum, coho, pink, and sockeye), including observations recorded as zero counts in the NuSEDS dataset.

![fig](`r paste0(wd_figures,"/Trends_populations_monitored_regions_species_WITH_0s.jpeg")`)
**Figure S5.** The average annual change in populations monitored since 1986 as estimated via linear regression for each species and region, including observations recorded as zero counts in the NuSEDS dataset. Dots and horizontal bars represent the slopes and their 95% confidence intervals, respectively. 

![fig](`r paste0(wd_figures,"/Fraser_Sockeye_NAs_0s.jpeg")`)
**Figure S6.** Pattern in zero counts (MAX_ESTIMATE) for Fraser sockeye populations (n = 351) in the cleaned NuSEDS data before filtering out zeros.

![fig](`r paste0(wd_figures,"/Barplot_EstimateClassification.jpeg")`)
**Figure S7.** Frequency of the different survey data quality values (ESTIMATE_CLASSIFICATION) in the cleaned NuSEDS data after filtering zeros.

![fig](`r paste0(wd_figures,"/Barplot_EstimateClassification_WITH_0s.jpeg")`)
**Figure S8.** Frequency of the different survey data quality values (ESTIMATE_CLASSIFICATION) in the cleaned NuSEDS data before filtering zeros.

![fig](`r paste0(wd_figures,"/Number_populations_monitored_species_rssq.jpeg")`)
**Figure S9.** Frequency of the different survey data quality values per species since 1998 in the cleaned NuSEDS data after filtering zeros. The stream survey quality grouping is based on the field ESTIMATE_CLASSIFICATION: “High” for “TRUE ABUNDANCE (TYPE-1)”; “Medium-High” for “TRUE ABUNDANCE (TYPE-2)”; “Medium” for “RELATIVE ABUNDANCE (TYPE-3)”; “Medium-Low” for “RELATIVE ABUNDANCE (TYPE-4)”; “Low” for “RELATIVE ABUNDANCE (TYPE-5)”, “PRESENCE-ABSENCE (TYPE-6)”, “PRESENCE/ABSENCE (TYPE-6)”, “RELATIVE: VARYING MULTI-YEAR METHODS” and “RELATIVE: CONSTANT MULTI-YEAR METHODS”.

![fig](`r paste0(wd_figures,"/Number_populations_monitored_species_rssq_WITH_0s.jpeg")`)
**Figure S10.** Frequency of the different survey data quality values per species since 1998 in the cleaned NuSEDS data before filtering zeros. The stream survey quality grouping is based on the field ESTIMATE_CLASSIFICATION: “High” for “TRUE ABUNDANCE (TYPE-1)”; “Medium-High” for “TRUE ABUNDANCE (TYPE-2)”; “Medium” for “RELATIVE ABUNDANCE (TYPE-3)”; “Medium-Low” for “RELATIVE ABUNDANCE (TYPE-4)”; “Low” for “RELATIVE ABUNDANCE (TYPE-5)”, “PRESENCE-ABSENCE (TYPE-6)”, “PRESENCE/ABSENCE (TYPE-6)”, “RELATIVE: VARYING MULTI-YEAR METHODS” and “RELATIVE: CONSTANT MULTI-YEAR METHODS”.

**Table S1.** Number of Conservation Units (CUs), populations, and sites present in the NuSEDS data in each region after cleaning (NAs counts were removed, zeros were kept). Sites are unique locations where spawners from one or more CUs may be counted. Populations are unique combinations of CUs and sites.

```{r, echo=FALSE}
filename <- "populationAssessed_catches_data_remove_NAs"
data_srg_0 <-  read_xlsx(paste0(wd_data_output,"/",filename,".xlsx"), 
                        sheet = "summary_regions") |> as.data.frame()
coln_new <- c("Region","Number of CUs","Number of populations","Number of sites")
colnames(data_srg_0)[1:length(coln_new)] <- coln_new

show <- data_srg_0[,coln_new]
rownames(show) <- NULL

library(knitr)
kable(show)
```


# Generealized linear regression

## Summary

The goal of the analysis is to quantify the effect of commercial catch on monitoring effort accounting for the effect of price over time and for each species.

We fit two models where the number of populations monitored (`count_pop`) is expressed as (1) a function of landing value, i.e., the product of commercial catch in kg (`catch_kg`) and commercial price per kg (`price_kg`), and (2) as a function of commercial catch alone. For both models we include `species` as a predictor and its interaction with the other predictor. We compare the two models using the Akaike information criterion (AIC).

We selected the negative binomial distribution to correct the overdispersion observed when using the Poisson distribution. The overdispersion is estimated by dividing the residual deviance by the residual degrees of freedom and it is < 1.1 for all four models (two models fitted on two datasets, each reasulting from from filtering or keeping the zeros in NuSEDS, respectively). We assessed collinearity among explanatory variables using pairwise plots and correlation coefficients. We used deviance residuals to verify the models’ assumptions with diagnostic plots (i.e., homoscedasticity of the residuals). We calculated a pseudo-R^2^ as the ratio between the null deviance minus the residual deviance divided by the null deviance.

## The datasets

Dataset containing the number of populations (`count_pop`) extracted from NuSEDS after filtering out the zeros:

```{r, echo=FALSE}
show <- head(data_sp)
rownames(show) <- NULL
show[,c("species","year","count_pop")]
```

Commercial Canadian catch data -- number of fish (`count`) and weight in kg (`wt_kg`) -- downloaded from the [North Pacific Anadromous Fish Commission website](https://www.npafc.org/statistics/) in July 2024:

```{r, echo=FALSE}
cond_sp <- catch$species == "Coho"
show <- head(catch[cond_sp,])
rownames(show) <- NULL
show
```

Price per kg data downloaded from [DFO’s 2023 Economic analysis](https://www.pac.dfo-mpo.gc.ca/analyses-econom-analysis/analyses/econ-perspective-salmon-saumon-eng.html):

```{r, echo=FALSE}
show <- head(price_sp)
rownames(show) <- NULL
show
```

These three datasets are combined into one for the analysis:

```{r, echo=FALSE}
show <- head(data_l$data)
rownames(show) <- NULL
show
```

The data range from `r min(data_l$data$year)` to `r max(data_l$data$year)`.

## Analysis excluding the zeros in NuSEDS

```{r, echo=FALSE, fig.width=8, fig.height=5}
#' Plot the time series on a standardised scale over time
species <- sort(unique(data_sp$species))

data_here <- data_l$data

price_min <- min(data_here$price_kg)
price_max <- max(data_here$price_kg)
catch_min <- min(data_here$catch_kg)
catch_max <- max(data_here$catch_kg)
count_min <- min(data_here$count_pop)
count_max <- max(data_here$count_pop)
year_min <- min(data_here$year)
year_max <- max(data_here$year)

layout(matrix(1:6, nrow = 2, byrow = T), heights = c(1,1.15), widths = c(1.1,1,1))
for(sp in species){
  # sp <- species[1]
  cond_sp <- data_here$species == sp
  count <- (data_here$count_pop[cond_sp] - count_min)/(count_max - count_min)
  catch_kg <- (data_here$catch_kg[cond_sp] - catch_min)/(catch_max - catch_min)
  price <- (data_here$price_kg[cond_sp] - price_min)/(price_max - price_min)
  years <- data_here$year[cond_sp]
  
  side1 <- side2 <- 2
  labels2 <- NA
  xaxt <- "n"
  if(which(sp == species) %in% c(1,4)){
    side2 <- 4.5
    labels2 <- c("min","max")
  }
  if(which(sp == species) %in% 4:5){
    side1 <- 4.5
    xaxt <- "s"
  }
  
  par(mar = c(side1,side2,.5,.5))
  plot(x = years, y = count, xlim = c(year_min,year_max), ylim = c(0,1.1), 
       ylab = "Standardised value", xlab = "Year", yaxt = "n", xaxt = xaxt,
       type = "l", lwd = 2,col = colour_transparency_fun("black",.5))
  axis(side = 2, at = 0:1,labels = labels2)
  if(!which(sp == species) %in% 4:6){
    axis(side = 1, labels = NA)
  }
  lines(x = years, y = price, lwd = 2, col = colour_transparency_fun("#9E163C",.5))
  lines(x = years, y = catch_kg, lwd = 2, col = colour_transparency_fun("#1962A0",.5))
  legend("topleft",sp,bty = "n")
}
plot(x = years, y = rep(1,length(years)), axes = FALSE, xlab = "", ylab = "", col = "white")
axis(side = 3, lwd.ticks = 0, col = "white", line = -.5)
mtext(text = "Year",side = 3, line = -1.3, cex = .7)
legend("center",c("number of populations monitored","catch","price"),
       col = c("black","#1962A0","#9E163C"), lwd = 2, bt = "n")
```

**Figure S11.** Number of populations monitored, catch and price over time for each species. Each variables was normalised across all species to be displayed on the same Y axis. Zeros counts were filtered out in NuSEDS.

### Check for collinearity

We check collinearity using pairwise plots comparing covariates and correlation coefficients:

```{r, echo=FALSE, fig.height=7, fig.width=10}
library(GGally)

ggpairs(data_here,                 # Data frame
        columns = which(colnames(data_here) %in% c("catch_kg","price_kg","species")),       # "count_pop",    # Columns
        aes(color =  species, alpha = 0.5)) +
  theme_bw()
```

**Figure S12.** Pairwise plot showing the distribution, relationship and correlation among the variables.

The correlations among explanatory variables are acceptable.

We notice that the explanatory variables catch (`catch_kg`) and price (`price_kg`) are right skewed, so we transform them with the square root function to improve the model fit:

```{r, echo=FALSE, fig.height=7, fig.width=10}
data_here$catch_kg_sqrt <- sqrt(data_here$catch_kg)
data_here$price_kg_sqrt <- sqrt(data_here$price_kg)

ggpairs(data_here,                 # Data frame
        columns = which(colnames(data_here) %in% c("catch_kg_sqrt","price_kg_sqrt","species")),           # Columns count_pop
        aes(color =  species, alpha = 0.5)) +
  theme_bw()
```

**Figure S13.** Pairwise plot showing the distribution, relationship and correlation among the variables with `catch` and `price` transformed with the square root function.

### Model definition

We first fit a GLM with a Poisson distribution and the log-link function:

```{r}
glm_cps <- glm(count_pop ~ catch_kg_sqrt:price_kg_sqrt * species, 
               data = data_here, family = poisson(link = "log"))

glm_cs <- glm(count_pop ~ catch_kg_sqrt * species, 
              data = data_here, family = poisson(link = "log"))
```

We estimate if there is overdispersion by dividing the residual deviance by the residual degrees of freedom:

```{r}
s_glm_cps <- summary(glm_cps)
round(s_glm_cps$deviance / s_glm_cps$df.residual,2)

s_glm_cs <- summary(glm_cs)
round(s_glm_cs$deviance / s_glm_cs$df.residual,2)
```
The values are much larger than 1, indicating overdispersion.

To solve this issue, we fit the same models but with a negative binomial distribution: 

```{r}
glm_cps <- glm.nb(count_pop ~ catch_kg_sqrt:price_kg_sqrt * species, 
               data = data_here, link = "log")

glm_cs <- glm.nb(count_pop ~ catch_kg_sqrt * species, 
              data = data_here, link = "log")
```

```{r}
s_glm_cps <- summary(glm_cps)
round(s_glm_cps$deviance / s_glm_cps$df.residual,2)

s_glm_cs <- summary(glm_cs)
round(s_glm_cs$deviance / s_glm_cs$df.residual,2)
```

There is no more overdispersion.

### Deviance residuals plots

We check the deviance residuals for `glm_cps`: 

```{r, echo=FALSE,fig.width=10}
span <- .9

glm_selected <- glm_cps

res <- resid(glm_selected, type = "deviance")

layout(matrix(1:6,nrow = 2, byrow = T))
par(mar = c(4.5,4.5,.5,.5))

fit <- predict(glm_selected)
plot(y = res, x = fit, xlab = "Predicted values", ylab = "Deviance residuals")
abline(a = 0,b = 0)
smoothed <- predict(loess(res ~ fit, span = span))
lines(x = fit[order(fit)], y = smoothed[order(fit)], col = "red")

qqnorm(res, main = "")
abline(a = 0, b = 1)

x <- data_here$catch_kg/1000000
plot(y = res, x = x, xlab = "Catch (in megatonne)", ylab = "Deviance residuals")
abline(0,0)
smoothed <- predict(loess(res ~ x, span = span))
lines(x = x[order(x)], y = smoothed[order(x)], col = "red")

#
x <- data_here$price_kg
plot(y = res, x = x, xlab = "Value (in CAD per kg)", ylab = "Deviance residuals")
abline(0,0)
smoothed <- predict(loess(res ~ x, span = span))
lines(x = x[order(x)], y = smoothed[order(x)], col = "red")

#
boxplot(res ~ data_here$species, xlab = "Species", ylab = "Deviance residuals", horizontal = F)
abline(0,0)
```

**Figure S14.** Residual diagnostic plots for the generalised linear model with a negative binomial distribution and log-link function to predict the number of populations monitored as a function of commercial catches, price and species (n = `r length(res)`).  Zero counts in NuSEDS were removed.

And for `glm_cs`:

```{r, echo=FALSE,fig.width=10}
span <- .9

glm_selected <- glm_cs

res <- resid(glm_selected, type = "deviance")

layout(matrix(1:6,nrow = 2, byrow = T))
par(mar = c(4.5,4.5,.5,.5))

fit <- predict(glm_selected)
plot(y = res, x = fit, xlab = "Predicted values", ylab = "Deviance residuals")
abline(a = 0,b = 0)
smoothed <- predict(loess(res ~ fit, span = span))
lines(x = fit[order(fit)], y = smoothed[order(fit)], col = "red")

qqnorm(res, main = "")
abline(a = 0, b = 1)

x <- data_here$catch_kg/1000000
plot(y = res, x = x, xlab = "Catch (in megatonne)", ylab = "Deviance residuals")
abline(0,0)
smoothed <- predict(loess(res ~ x, span = span))
lines(x = x[order(x)], y = smoothed[order(x)], col = "red")

#
x <- data_here$price_kg
plot(y = res, x = x, xlab = "Value (in CAD per kg)", ylab = "Deviance residuals")
abline(0,0)
smoothed <- predict(loess(res ~ x, span = span))
lines(x = x[order(x)], y = smoothed[order(x)], col = "red")

#
boxplot(res ~ data_here$species, xlab = "Species", ylab = "Deviance residuals", horizontal = F)
abline(0,0)
```

**Figure S15.** Residual diagnostic plots for the generalised linear model with a negative binomial distribution and log-link function to predict the number of populations monitored as a function of commercial catches and species (n = `r length(res)`).  Zero counts in NuSEDS were removed.

The residuals plots do not invalidate both models.

### Model comparison

We compare the two models using AIC and pseudo-R^2^. The pseudo-R^2^ is expressed as the ratio between the null deviance minus the residual deviance divided by the null deviance.

```{r, echo=FALSE}
show <- data.frame(model = c("glm_cps","glm_cs"))

show$AIC <- lapply(list(glm_cps,glm_cs),function(m){
    return(round(AIC(m),1))
  }) |> unlist()

# show$AICc <- lapply(list(glm_cps,glm_cs),function(m){
#     return(AICc(m))
#   }) |> unlist()

show$R2_pseudo <- lapply(list(glm_cps,glm_cs),function(m){
    sm <- summary(m)
    out <- (sm$null.deviance - sm$deviance)/sm$null.deviance
    return(round(out,3))
  }) |> unlist()

show
```

The model expressing the effect of landed value (i.e,, the product of catch `catch_kg` and price `price_kg`) on the number of population monitored (`glm_cps`) is the best model according to AIC (delta AIC = `r show$AIC[2] - show$AIC[1]`).


### Parameter estimates

The parameter estimates for `glm_cps` are:

```{r, echo=FALSE}
summary_l <- list()
count <- 1
for(sm in list(s_glm_cps,s_glm_cs)){
  sm <- sm[["coefficients"]]
  
  out <- data.frame(parameters = rownames(sm),
                    estimate = round(sm[,"Estimate"],3),
                    SE = round(sm[,"Std. Error"],4),
                    P_value = round(sm[,"Pr(>|z|)"],3))
  
  out$P_value <- sapply(sm[,"Pr(>|z|)"],function(pv){
    out <- "."
    if(pv < 0.001){
      out <- "< 0.001"
    }else if(pv < 0.01){
      out <- "< 0.01"
    }else if(pv < 0.05){
      out <- "< 0.05"
    }else{
      out <- round(pv,2)
    }
    return(out)
  })
  rownames(out) <- NULL
  summary_l[[count]] <- out
  count <- count + 1
}

names(summary_l) <- c("glm_cps","glm_cs")
summary_l$glm_cps
```

The parameter estimates for `glm_cs` are:

```{r,echo=FALSE}
summary_l$glm_cs
```

### Predictions

We predict *number of populations monitored* (`count_pop`) as a function of *Landed value* = *catch* (`catch_kg`)  $\times$ *price* (`price_kg`) for each species using the `glm_cps` model:

```{r, echo=FALSE, fig.height=6, fig.width=9}

suffix <- "WHITOUT_0s"
count <- 1

model_selected <- glm_cps

y_max <- max(data_here$count_pop)
y_min <- min(data_here$count_pop)

colfunc_yr <- colorRampPalette(c("#9E163C","#1962A0"))
col_points <- colour_transparency_fun(colfunc_yr(length(years)),.5)
names(col_points) <- years

if(figures_print){
  jpeg(paste0(wd_figures,"/Predictinon_glm_",suffix,".jpeg"),
       width = 20, height = 15, units = 'cm', res = 300)
}
layout(mat = matrix(1:6, nrow = 2, byrow = T), widths = c(1.22,1,1), heights = c(1,1.07))
for(sp in species){
  # sp <- "Chum"
  
  cond_sp <- data_here$species == sp
  
  data_glm <- data_here[cond_sp,]
  
  data_glm$landing_value <- data_glm$catch_kg * data_glm$price_kg
  data_glm <- data_glm[order(data_glm$landing_value),]
  
  x_max <- max(data_glm$landing_value)
  x_min <- min(data_glm$landing_value)
  
  side1 <- 3
  side2 <- .5
  xlabel <- ylabel <- ""
  yaxt <- "n"
  if(count %in% c(1,4)){
    side2 <- 4.5
    yaxt <- "s"
    ylabel <- "Number of population monitored"
  }
  if(count > 3){
    side1 <- 4.5
    xlabel <- "Landed value (CAD M)"
  }
  
  par(mar = c(side1, side2, .5, .5))
  plot(NA,xlim = c(x_min,x_max),ylim = c(y_min,y_max), xaxt = "n", yaxt = yaxt,
       xlab = xlabel, ylab = ylabel)
  xticks <- axTicks(1)
  axis(side = 1, at = xticks, labels = xticks / 1000000)
  
  # data points
  points(x = data_glm$landing_value,y = data_glm$count_pop, 
         col = col_points[as.character(data_glm$year)], pch = 16 ,cex = 1.5)
  
  legend("topleft",legend = paste0(letters[count],") ",sp), bty = "n")
  
  # legend("bottomleft",paste0("n = ",sum(cond_sp)), bty = "n")
  
  fit_se <- predict(object = model_selected,
                    newdata = data_glm, 
                    type = "response", se.fit = T)
  
  CI_low <- fit_se$fit - 1.96 * fit_se$se.fit
  CI_high <- fit_se$fit + 1.96 * fit_se$se.fit
  
  polygon(x = c(data_glm$landing_value,rev(data_glm$landing_value)), 
          y = c(CI_low,rev(CI_high)), border = NA,
          col = colour_transparency_fun("grey35",alpha = .2))
  
  lines(x = data_glm$landing_value, y = fit_se$fit, 
        col = colour_transparency_fun("grey10",alpha = .7), lwd = 2)
  
  count <- count + 1
}
plot(1, type = "n", axes = FALSE, xlab = "", ylab = "", xlim = c(0,4))
yrs_points <- c("1960","1980","2000","2020")
x_points <- c(1,1.7,2.4,3.1)
text(x = 2, y = 1.1, labels = "Years", cex = 1.5)
points(x = x_points, y = rep(1,4), pch = 16, cex = 2, col = col_points[yrs_points])
text(x = x_points, y = rep(.95,4), labels = yrs_points)
if(figures_print){
  dev.off()
}
```

**Figure S16.** Prediction of the number of populations monitored as a function of landed value (in million of CAD), which is the product between commercial catch (in kg) and price (in CAD per kg). A generalized linear model with a negative binomial distribution and log-link function was fitted with catch, price and species as predictive variables  (n = `r nrow(data_here)`). Lines and polygons represent model fits and 95% confidence intervals, respectively; circles represent the data, which span from `r min(data_here$year)` and `r max(data_here$year)`. Zero counts in NuSEDS were filtered out. This figure is the one appearing in the main text.

## Analysis including the zeros in NuSEDS

We use here the equivalent dataset but where zeros were considered as positive observation to assess the number of population monitored in a given year:

```{r, echo=FALSE}
show <- head(data_l$data_0)
rownames(show) <- NULL
show
```

```{r, echo=FALSE, fig.width=8, fig.height=5}
#' Plot the time series on a standardised scale over time

data_here <- data_l$data_0

price_min <- min(data_here$price_kg)
price_max <- max(data_here$price_kg)
catch_min <- min(data_here$catch_kg)
catch_max <- max(data_here$catch_kg)
count_min <- min(data_here$count_pop)
count_max <- max(data_here$count_pop)
year_min <- min(data_here$year)
year_max <- max(data_here$year)

layout(matrix(1:6, nrow = 2, byrow = T), heights = c(1,1.15), widths = c(1.1,1,1))
for(sp in species){
  # sp <- species[1]
  cond_sp <- data_here$species == sp
  count <- (data_here$count_pop[cond_sp] - count_min)/(count_max - count_min)
  catch_kg <- (data_here$catch_kg[cond_sp] - catch_min)/(catch_max - catch_min)
  price <- (data_here$price_kg[cond_sp] - price_min)/(price_max - price_min)
  years <- data_here$year[cond_sp]
  
  side1 <- side2 <- 2
  labels2 <- NA
  xaxt <- "n"
  if(which(sp == species) %in% c(1,4)){
    side2 <- 4.5
    labels2 <- c("min","max")
  }
  if(which(sp == species) %in% 4:5){
    side1 <- 4.5
    xaxt <- "s"
  }
  
  par(mar = c(side1,side2,.5,.5))
  plot(x = years, y = count, xlim = c(year_min,year_max), ylim = c(0,1.1), 
       ylab = "Standardised value", xlab = "Year", yaxt = "n", xaxt = xaxt,
       type = "l", lwd = 2,col = colour_transparency_fun("black",.5))
  axis(side = 2, at = 0:1,labels = labels2)
  if(!which(sp == species) %in% 4:6){
    axis(side = 1, labels = NA)
  }
  
  lines(x = years, y = price, lwd = 2, col = colour_transparency_fun("#9E163C",.5))
  lines(x = years, y = catch_kg, lwd = 2, col = colour_transparency_fun("#1962A0",.5))
  
  # show the counts without 0s
  cond_sp <- data_l$data$species == sp
  count_wo0 <- (data_l$data$count_pop[cond_sp] - count_min)/(count_max - count_min)
  lines(x = years, y = count_wo0, lwd = 2, col = colour_transparency_fun("grey30",.5))
  
  legend("topleft",sp,bty = "n")
}
plot(x = years, y = rep(1,length(years)), axes = FALSE, xlab = "", ylab = "", col = "white")
axis(side = 3, lwd.ticks = 0, col = "white", line = -.5)
mtext(text = "Year",side = 3, line = -1.3, cex = .7)
legend("center",c("nb. populations monitored (with 0s)","nb. populations monitored (without 0s)","catch","price"),
       col = c("black","grey30","#1962A0","#9E163C"), lwd = 2, bt = "n")
```

**Figure S17**: Number of populations monitored, catch and price over time for each species. Each variables was normalised across all species to be displayed on the same Y axis. Zeros counts were not filtered out in NuSEDS (also the figure shows the number).

### Check for collinearity

We check collinearity using pairwise plots comparing covariates and correlation coefficients (after transforming *catch* `catch_kg` and *price* `price_kg` with the square root function):

```{r, echo=FALSE, fig.height=7, fig.width=10}
data_here$catch_kg_sqrt <- sqrt(data_here$catch_kg)
data_here$price_kg_sqrt <- sqrt(data_here$price_kg)

ggpairs(data_here,                 # Data frame
        columns = which(colnames(data_here) %in% c("catch_kg_sqrt","price_kg_sqrt","species")),           # Columns count_pop
        aes(color =  species, alpha = 0.5)) +
  theme_bw()
```

**Figure S18**: Pairwise plot showing the distribution, relationship and correlation among the variables with `catch` and `price` transformed with the square root function.

The correlations among explanatory variables are acceptable.


### Model definition

We first fit a GLM with a Poisson distribution and the log-link function:

```{r}
glm_cps <- glm(count_pop ~ catch_kg_sqrt:price_kg_sqrt * species, 
               data = data_here, family = poisson(link = "log"))

glm_cs <- glm(count_pop ~ catch_kg_sqrt * species, 
              data = data_here, family = poisson(link = "log"))
```

We estimate if there is overdispersion by dividing the residual deviance by the residual degrees of freedom:

```{r}
s_glm_cps <- summary(glm_cps)
round(s_glm_cps$deviance / s_glm_cps$df.residual,2)

s_glm_cs <- summary(glm_cs)
round(s_glm_cs$deviance / s_glm_cs$df.residual,2)
```
The values are much larger than 1, indicating overdispersion.

To solve this issue, we fit the same models but with a negative binomial distribution: 

```{r}
glm_cps <- glm.nb(count_pop ~ catch_kg_sqrt:price_kg_sqrt * species, 
               data = data_here, link = "log")

glm_cs <- glm.nb(count_pop ~ catch_kg_sqrt * species, 
              data = data_here, link = "log")
```

```{r}
s_glm_cps <- summary(glm_cps)
round(s_glm_cps$deviance / s_glm_cps$df.residual,2)

s_glm_cs <- summary(glm_cs)
round(s_glm_cs$deviance / s_glm_cs$df.residual,2)
```

There is no more overdispersion.

### Deviance residuals plots

We check the deviance residuals for `glm_cps`: 

```{r, echo=FALSE, fig.width=10}
span <- .9

glm_selected <- glm_cps

res <- resid(glm_selected, type = "deviance")

layout(matrix(1:6,nrow = 2, byrow = T))
par(mar = c(4.5,4.5,.5,.5))

fit <- predict(glm_selected)
plot(y = res, x = fit, xlab = "Predicted values", ylab = "Deviance residuals")
abline(a = 0,b = 0)
smoothed <- predict(loess(res ~ fit, span = span))
lines(x = fit[order(fit)], y = smoothed[order(fit)], col = "red")

qqnorm(res, main = "")
abline(a = 0, b = 1)

x <- data_here$catch_kg/1000000
plot(y = res, x = x, xlab = "Catch (in megatonne)", ylab = "Deviance residuals")
abline(0,0)
smoothed <- predict(loess(res ~ x, span = span))
lines(x = x[order(x)], y = smoothed[order(x)], col = "red")

#
x <- data_here$price_kg
plot(y = res, x = x, xlab = "Value (in CAD per kg)", ylab = "Deviance residuals")
abline(0,0)
smoothed <- predict(loess(res ~ x, span = span))
lines(x = x[order(x)], y = smoothed[order(x)], col = "red")

#
boxplot(res ~ data_here$species, xlab = "Species", ylab = "Deviance residuals", horizontal = F)
abline(0,0)
```

**Figure S20.** Residual diagnostic plots for the generalised linear model with a negative binomial distribution and log-link function to predict the number of populations monitored as a function of commercial catches, price and species (n = `r length(res)`).  Zero counts in NuSEDS were kept.

And for `glm_cs`:

```{r, echo=FALSE,fig.width=10}
span <- .9

glm_selected <- glm_cs

res <- resid(glm_selected, type = "deviance")

layout(matrix(1:6,nrow = 2, byrow = T))
par(mar = c(4.5,4.5,.5,.5))

fit <- predict(glm_selected)
plot(y = res, x = fit, xlab = "Predicted values", ylab = "Deviance residuals")
abline(a = 0,b = 0)
smoothed <- predict(loess(res ~ fit, span = span))
lines(x = fit[order(fit)], y = smoothed[order(fit)], col = "red")

qqnorm(res, main = "")
abline(a = 0, b = 1)

x <- data_here$catch_kg/1000000
plot(y = res, x = x, xlab = "Catch (in megatonne)", ylab = "Deviance residuals")
abline(0,0)
smoothed <- predict(loess(res ~ x, span = span))
lines(x = x[order(x)], y = smoothed[order(x)], col = "red")

#
x <- data_here$price_kg
plot(y = res, x = x, xlab = "Value (in CAD per kg)", ylab = "Deviance residuals")
abline(0,0)
smoothed <- predict(loess(res ~ x, span = span))
lines(x = x[order(x)], y = smoothed[order(x)], col = "red")

#
boxplot(res ~ data_here$species, xlab = "Species", ylab = "Deviance residuals", horizontal = F)
abline(0,0)
```

**Figure S21.** Residual diagnostic plots for the generalised linear model with a negative binomial distribution and log-link function to predict the number of populations monitored as a function of commercial catches and species (n = `r length(res)`).  Zero counts in NuSEDS were kept.

The residuals plots do not invalidate both models.

### Model comparison

We compare the two models using AIC and pseudo-R^2^. The pseudo-R^2^ is expressed as the ratio between the null deviance minus the residual deviance divided by the null deviance.

```{r, echo=FALSE}
show <- data.frame(model = c("glm_cps","glm_cs"))

show$AIC <- lapply(list(glm_cps,glm_cs),function(m){
    return(round(AIC(m),1))
  }) |> unlist()

# show$AICc <- lapply(list(glm_cps,glm_cs),function(m){
#     return(AICc(m))
#   }) |> unlist()

show$R2_pseudo <- lapply(list(glm_cps,glm_cs),function(m){
    sm <- summary(m)
    out <- (sm$null.deviance - sm$deviance)/sm$null.deviance
    return(round(out,3))
  }) |> unlist()

show
```

The model expressing the effect of landed value (i.e,, the product of catch `catch_kg` and price `price_kg`) on the number of population monitored (`glm_cps`) is the best model according to AIC (delta AIC = `r show$AIC[2] - show$AIC[1]`).


### Parameter estimates

The parameter estimates for `glm_cps` are:

```{r, echo=FALSE}
summary_l <- list()
count <- 1
for(sm in list(s_glm_cps,s_glm_cs)){
  sm <- sm[["coefficients"]]
  
  out <- data.frame(parameters = rownames(sm),
                    estimate = round(sm[,"Estimate"],3),
                    SE = round(sm[,"Std. Error"],4),
                    P_value = round(sm[,"Pr(>|z|)"],3))
  
  out$P_value <- sapply(sm[,"Pr(>|z|)"],function(pv){
    out <- "."
    if(pv < 0.001){
      out <- "< 0.001"
    }else if(pv < 0.01){
      out <- "< 0.01"
    }else if(pv < 0.05){
      out <- "< 0.05"
    }else{
      out <- round(pv,2)
    }
    return(out)
  })
  rownames(out) <- NULL
  summary_l[[count]] <- out
  count <- count + 1
}

names(summary_l) <- c("glm_cps","glm_cs")
summary_l$glm_cps
```

The parameter estimates for `glm_cs` are:

```{r,echo=FALSE}
summary_l$glm_cs
```

### Predictions

We predict *number of populations monitored* (`count_pop`) as a function of *Landed value* = *catch* (`catch_kg`)  $\times$ *price* (`price_kg`) for each species using the `glm_cps` model:

```{r, echo=FALSE, fig.height=6, fig.width=9}

suffix <- "WHITH_0s" # not used here
count <- 1

model_selected <- glm_cps

y_max <- max(data_here$count_pop)
y_min <- min(data_here$count_pop)

colfunc_yr <- colorRampPalette(c("#9E163C","#1962A0"))
col_points <- colour_transparency_fun(colfunc_yr(length(years)),.5)
names(col_points) <- years

if(figures_print){
  jpeg(paste0(wd_figures,"/Predictinon_glm_",suffix,".jpeg"),
       width = 20, height = 15, units = 'cm', res = 300)
}
layout(mat = matrix(1:6, nrow = 2, byrow = T), widths = c(1.22,1,1), heights = c(1,1.07))
for(sp in species){
  # sp <- "Chum"
  
  cond_sp <- data_here$species == sp
  
  data_glm <- data_here[cond_sp,]
  
  data_glm$landing_value <- data_glm$catch_kg * data_glm$price_kg
  data_glm <- data_glm[order(data_glm$landing_value),]
  
  x_max <- max(data_glm$landing_value)
  x_min <- min(data_glm$landing_value)
  
  side1 <- 3
  side2 <- .5
  xlabel <- ylabel <- ""
  yaxt <- "n"
  if(count %in% c(1,4)){
    side2 <- 4.5
    yaxt <- "s"
    ylabel <- "Number of population monitored"
  }
  if(count > 3){
    side1 <- 4.5
    xlabel <- "Landed value (CAD M)"
  }
  
  par(mar = c(side1, side2, .5, .5))
  plot(NA,xlim = c(x_min,x_max),ylim = c(y_min,y_max), xaxt = "n", yaxt = yaxt,
       xlab = xlabel, ylab = ylabel)
  xticks <- axTicks(1)
  axis(side = 1, at = xticks, labels = xticks / 1000000)
  
  # data points
  points(x = data_glm$landing_value,y = data_glm$count_pop, 
         col = col_points[as.character(data_glm$year)], pch = 16 ,cex = 1.5)
  
  legend("topleft",legend = paste0(letters[count],") ",sp), bty = "n")
  
  # legend("bottomleft",paste0("n = ",sum(cond_sp)), bty = "n")
  
  fit_se <- predict(object = model_selected,
                    newdata = data_glm, 
                    type = "response", se.fit = T)
  
  CI_low <- fit_se$fit - 1.96 * fit_se$se.fit
  CI_high <- fit_se$fit + 1.96 * fit_se$se.fit
  
  polygon(x = c(data_glm$landing_value,rev(data_glm$landing_value)), 
          y = c(CI_low,rev(CI_high)), border = NA,
          col = colour_transparency_fun("grey35",alpha = .2))
  
  lines(x = data_glm$landing_value, y = fit_se$fit, 
        col = colour_transparency_fun("grey10",alpha = .7), lwd = 2)
  
  count <- count + 1
}
plot(1, type = "n", axes = FALSE, xlab = "", ylab = "", xlim = c(0,4))
yrs_points <- c("1960","1980","2000","2020")
x_points <- c(1,1.7,2.4,3.1)
text(x = 2, y = 1.1, labels = "Years", cex = 1.5)
points(x = x_points, y = rep(1,4), pch = 16, cex = 2, col = col_points[yrs_points])
text(x = x_points, y = rep(.95,4), labels = yrs_points)
if(figures_print){
  dev.off()
}
```

**Figure S22.** Prediction of the number of populations monitored as a function of landed value (in million of CAD), which is the product between commercial catch (in kg) and price (in CAD per kg). A generalized linear model with a negative binomial distribution and log-link function was fitted with catch, price and species as predictive variables (n = `r nrow(data_here)`). Lines and polygons represent model fits and 95% confidence intervals, respectively; circles represent the data, which span from `r min(data_here$year)` and `r max(data_here$year)`. Zero counts in NuSEDS were kept.

# NuSEDS Data Processing

The R script containing the code for the NuSEDS data processing procedure described below is available in a Zenodo repository (https://zenodo.org/records/14194638). Complete details of the cleaning procedure can be found at: https://bookdown.org/salmonwatersheds/nuseds_cleaning_procedure_atkinson/1_nuseds_collation_1.html.

## Cleaning Procedure

The objective of the procedure is to obtain the yearly counts of each salmon population in their respective stream and associate these populations to their corresponding conservation unit (CU). The NuSEDS data is separated into two datasets. The all_areas_nuseds dataset contains the observed yearly counts (related fields: `NATURAL_ADULT_SPAWNERS`, `NATURAL_SPAWNERS_TOTAL`, etc.) for each population (related fields: `SPECIES`, `POP_ID`, `POPULATION`) in their respective site (related fields: `AREA`, `GFE_ID`, `WATERBODY`, `GAZETTED_NAME`, etc.), along with the associated methods used (related fields: `ESTIMATE_METHOD`, `ESTIMATE_CLASSIFICATION`, `ENUMERATION_METHODS`). 

We first define the unique yearly count field MAX_ESTIMATE for each population as the maximum value of the count-related fields in all_areas_nuseds, i.e., `NATURAL_ADULT_SPAWNERS`, `NATURAL_JACK_SPAWNERS`, `NATURAL_SPAWNERS_TOTAL`, `ADULT_BROODSTOCK_REMOVALS`, `JACK_BROODSTOCK_REMOVALS`, `TOTAL_BROODSTOCK_REMOVALS`, `OTHER_REMOVALS` and `TOTAL_RETURN_TO_RIVER`. `MAX_ESTIMATE` is the only count-related field we use in the rest of the procedure. A population’s `MAX_ESTIMATE` data points is referred to as its “time series”.

The second dataset, conservation_unit_system_sites, links each population (related fields: POP_ID) to their respective CU (related fields: CU_NAME, FULL_CU_IN, CU_LONGT, CU_LAT, etc.) and site (related fields: GFE_ID, SYSTEM_SITE, X_LONGT, Y_LAT). Ideally, attributing each time series in all_areas_nuseds its corresponding CU and location’s coordinates in conservation_unit_system_sites would simply consist in merging the two datasets using the population and location identification number POP_ID and GFE_ID, respectively. Unfortunately, numerous time series are problematic, which occurs when:

- A time series is present in all_areas_nuseds but its POP_ID and GFE_ID association is absent in conservation_unit_system_sites (there are 4428 populations in that case).

- Multiple time series of the same population (i.e. same `POP_ID`) are observed in multiple locations (i.e. different `GFE_IDs`), which should not occur because a POP_ID should be defined for a unique location.

- Multiple populations (i.e. different `POP_ID`) of a same CU are observed in the same location (i.e. same `GFE_ID`), suggesting that these populations should form one unique population.

The observation of problematic time series revealed inconsistencies such as missing, duplicated, and conflicting data points (i.e. different counts in the same year). In order to rescue as much data as possible, we attempted to solve these issues by either *(i)* **deleting**, *(ii)* **combining** or *(iii)* **summing** a problematic series to an **alternative series** (i.e., a time series present in all_areas_nuseds that shares either the same `POP_ID` or `GFE_ID` and species), or *(iv)* **creating** a new reference to the time series in conservation_unit_system_sites. In each case, we used all the information available in the different fields and made assumptions based on our professional judgment. Below are more details about the different interventions applied to problematic time series:

- We **deleted** a problematic time series (or a portion of it) from all_areas_nuseds when (1) it was a duplicate of another time series, (2) it had less than four data points and was not spotted during the cleaning process, or (3) the lack of information prevented to combine or merge it to an alternative series or to create a new series in conservation_unit_system_sites (e.g. if run timing was unknown).

- We **combined** a problematic time series with an alternative series when data points were not in conflict (i.e. different value for a same year) and the information available in the other fields was not prescriptive (e.g. the population belong to the same CU, the locations have similar names or spatial coordinates, the runtimings are not different).

- We **summed** a problematic time series to an alternative series having different `POP_ID` but same `GFE_ID` and `CU_NAME` and with conflicting data points.

- We **created** a new reference of the problematic time series in conservation_unit_system_sites (i.e. a new row with a unique `POP_ID` - `GFE_ID` combination) when it could not be deleted, merged or combined to an alternative series.

The diverse and idiosyncratic nature of these problematic cases prevented the establishment of a systematic and simple cleaning process. Many of these cases were resolved individually by observing and comparing time series and accounting for the information available. We proceeded in consecutive steps in which we applied the different types of intervention mentioned above, either after visually inspecting individual time series or by applying a systematic procedure. These steps are detailed below:

1. Remove the time series (i.e. unique `POP_ID` - `GFE_ID` associations) with only NAs values for `MAX_ESTIMATE` in all_areas_nuseds (corresponding to 4424 time series, 101,944 rows or 25% of all_areas_nuseds)

2. Assess all the references to time series present in conservation_unit_system_sites but absent in all_areas_nuseds (247 out of 714 time series in all_areas_nuseds are not referenced in conservation_unit_system_sites)

  + 2.a. Cases with presence of alternative series in all_areas_nuseds (n = 3)
  
    - Each case was assessed visually.

  + 2.b. Cases with no alternative time series in all_areas_nuseds (n = 243)
  
    - The rows corresponding to these time series are removed from conservation_unit_system_sites (242 of the 243 references correspond to time series in all_areas_nuseds removed in step 1. above because they contained only NAs).

  + 2.c. Case with multiple `GFE_ID` for a single `POP_ID` (n = 1)
  
    - The problematic time series was simply removed from conservation_unit_system_sites.

3. Case of time series with the same `POP_ID` but different `GFE_ID`s in conservation_unit_system_sites and present in all_areas_nuseds (n = 1).

  - Two time series had the same `POP_ID` but different `GFE_ID`. Both time series were kept because they are not compatible and have many and recent data points.

4. Assess all the time series present in all_areas_nuseds but absent in conservation_unit_system_sites (n = 242)

  + 4.a. Cases of series with alternative `POP_ID` and no alternative `GFE_ID` (n = 126)

    - Each case was assessed visually.

  + 4.b. Cases of series with alternative `GFE_ID` with or without alternative `POP_ID` (n = 64)

    - Problematic series with the same `POP_ID`s (e.g. series with `POP_ID` = 47925 and `GFE_ID` = 15, 31516 and 31740, respectively) having the same alternative series (e.g. series with `POP_ID` = 47925 and `GFE_ID` = 14) are assessed together, resulting in 37 groups of time series to assess separately.

  + 4.c. Cases with no alternative series (n = 166)
  
    - We attempted to associate these time series to a CU using the fields `X_LONGT` and `Y_LAT` to intersect them with the CUs’ shapefiles used in the PSE.

    - Time series without coordinates (or for which coordinates could not be found) were removed from all_areas_nuseds (n = 5).
 
    - The reference of the rest of the time series was added to conservation_unit_system_sites

5.  Merge all_areas_nuseds and conservation_unit_system_sites using the fields `POP_ID` and `GFE_ID`.

6.  Assess time series with different `POP_ID` but a same `CU_NAME` and `GFE_ID` (n = 126).

  - Case 1: there is only one data point in one of the series; if it is compatible, then merge to the alternative series, otherwise remove it from all_areas_nuseds.

  - Case 2: One of the series is 100% duplicated with the other one: to remove from all_areas_nuseds.

  - Case 3: the rest of the series are merged; adding the values when data points are in conflict.


## Attribution of the geographic regions to CUs

We matched the CUs in the cleaned NuSEDS data with the CUs defined in the Pacific Salmon Explorer, which are associated with a given region (PSE; https://salmonexplorer.ca). We used the fields `POP_ID`, `FULL_CU_IN` and `CU_NAME`. There are 23 CU_NAME not matching to any CUs in the PSE, corresponding to 87 populations and 2256 data points. These time series are kept.

## Managing NA and zero counts

For the analysis presented in the main text, we excluded data points where `MAX_ESTIMATE` = NA (n = 156,507) or 0 (n = 3,449) and we present here in the appendix the results of the analyses done on the cleaned NuSEDS dataset where zeros are kept.



